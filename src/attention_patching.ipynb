{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import seaborn as sns\n",
    "import json\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "# another code we wrote\n",
    "sys.path.append('../')\n",
    "import llm_utils\n",
    "import opt_utils\n",
    "from plot_utils import plot_aux_wrapper\n",
    "import exp_ra_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from function_vectors.src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "except Exception as error:\n",
    "    print('could not import from function_vectors package with the following error:')\n",
    "    print(error)\n",
    "    print('Make sure you first pull relevant submodules. See README.md for more info.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "    DEBUG MODE [2024/09/28-16:53:17]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "START_TIME = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "# DEBUG_FLAG = True  # for local testing\n",
    "DEBUG_FLAG = False  # automatically set below\n",
    "\n",
    "try:\n",
    "    DEBUG_FLAG = torch.backends.mps.is_available()  # since only Apple M1-3 supports this, we can use this as a flag for local testing\n",
    "except:\n",
    "    pass\n",
    "if DEBUG_FLAG:\n",
    "    print('*'*40)\n",
    "    print(f'    DEBUG MODE [{START_TIME}]')\n",
    "    print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='gpt2' if DEBUG_FLAG else 'gpt2-xl')\n",
    "parser.add_argument('--model_args', type=str, default='')\n",
    "parser.add_argument('--layer_opt_filter', type=str, default='attn_only')\n",
    "parser.add_argument('--out_folder', type=str, default='tmp_plots1')\n",
    "parser.add_argument('--postfix_name', type=str, default='')\n",
    "parser.add_argument('--disable_pad_token', action='store_true')\n",
    "parser.add_argument('--root_data_dir', type=str, default='../function_vectors/dataset_files')\n",
    "parser.add_argument('--dataset_name', type=str, default='antonym_len_8')\n",
    "parser.add_argument('--n_shots_icl', type=str, default='0,5' if DEBUG_FLAG else '0,1,5,10')\n",
    "parser.add_argument('--n_samples', type=int, default=5 if DEBUG_FLAG else 25)\n",
    "parser.add_argument('--metric_to_use', type=str, default='f1_score')  # f1_score, exact_match_score, first_word_score. not really used as we examine only accuracy of top-{1,2,3} predictions\n",
    "parser.add_argument('--prefixes', help='Prompt template prefixes to be used', type=json.loads, required=False, default={\"input\":\"Q:\", \"output\":\"A:\", \"instructions\":\"\"})\n",
    "parser.add_argument('--separators', help='Prompt template separators to be used', type=json.loads, required=False, default={\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"})    \n",
    "parser.add_argument('--device', type=str, default=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--check_if_result_already_exists', action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown = parser.parse_known_args()\n",
    "print('unknown args:', unknown)\n",
    "print('args:', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "if not args.disable_pad_token:\n",
    "    print(f'adding pad token: {tokenizer.eos_token}')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "try:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # not blocking, just to prevent warnings and faster tokenization\n",
    "except:\n",
    "    pass\n",
    "padding_flag = tokenizer.pad_token_id is not None  # this is up to if the tokenizer was configured with padding or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu [cuda available? => False, cuda version: None, args.device = \"cpu\"]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "print(f'Using device: {device} [cuda available? => {torch.cuda.is_available()}, cuda version: {torch.version.cuda}, args.device = \"{args.device}\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_extra_args: {}\n",
      "Loading config from /Users/ks/Documents/research/projD2/opt07/bl2/clean_bu_code_v2/flow_graph_configs/config_gpt2.json\n",
      "{'config_name': 'gpt2', 'layer_format': 'transformer.h.{}', 'layer_mlp_format': 'transformer.h.{}.mlp', 'layer_attn_format': 'transformer.h.{}.attn', 'ln1': 'transformer.h.{}.ln_1', 'attn_q': 'transformer.h.{}.attn.c_attn', 'attn_k': 'transformer.h.{}.attn.c_attn', 'attn_v': 'transformer.h.{}.attn.c_attn', 'attn_o': 'transformer.h.{}.attn.c_proj', 'ln2': 'transformer.h.{}.ln_2', 'mlp_ff1': 'transformer.h.{}.mlp.c_fc', 'mlp_ff2': 'transformer.h.{}.mlp.c_proj', 'mlp_act': 'transformer.h.{}.mlp.act', 'include_mlp_bias': True, 'include_attn_bias': True}\n",
      "n_shots_icl: [0, 5]\n"
     ]
    }
   ],
   "source": [
    "model_extra_args = {}\n",
    "for arg in args.model_args.split(','):\n",
    "    if arg == '':\n",
    "        continue\n",
    "    k, v = arg.split('=')\n",
    "    model_extra_args[k] = v\n",
    "print(f'model_extra_args: {model_extra_args}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_extra_args).eval().requires_grad_(False).to(device)\n",
    "model_aux = llm_utils.model_extra(model=model, device=device)\n",
    "model_config = copy.deepcopy(model.config)\n",
    "config = model_aux.config\n",
    "# del model\n",
    "\n",
    "n_embd = model_aux.n_embd\n",
    "n_head = model_aux.n_head\n",
    "head_size = model_aux.head_size\n",
    "n_layer = model_aux.n_layer\n",
    "\n",
    "pad_k = model_aux.pad_k\n",
    "pad_v = model_aux.pad_v\n",
    "\n",
    "\n",
    "# params_names_filter = opt_utils.get_filter_by_name(args.filter_layers)\n",
    "if args.layer_opt_filter == 'attn_only':\n",
    "    params_names_filter = lambda x: ('attn' in x or 'attention' in x) and 'weight' in x and 'layer_norm' not in x\n",
    "     # tested for attn with gpt2 and opt\n",
    "else:\n",
    "    params_names_filter = opt_utils.get_filter_by_name(args.layer_opt_filter)  # tested for attn with gpt2 and opt\n",
    "# params_names_filter = lambda x: ('attn' in x or 'attention' in x) and 'weight' in x\n",
    "\n",
    "n_shots_icl = [int(x) for x in args.n_shots_icl.split(',')]\n",
    "print(f'n_shots_icl: {n_shots_icl}')\n",
    "metric_to_use = args.metric_to_use\n",
    "n_samples = args.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prefix is a code for which version of the code we use in this notebook\n",
    "output_prefix = f'ap1_{args.model_name.replace(\"/\", \"-\")}_{args.dataset_name}_[{args.n_shots_icl}]_{n_samples}_{args.postfix_name}_'\n",
    "plot_wrapper = plot_aux_wrapper(output_folder=args.out_folder, \n",
    "                            output_prefix=output_prefix,\n",
    "                            local_show=True)\n",
    "\n",
    "show_every_n_layer = 1 if n_layer <= 12 else 2 if n_layer <= 24 else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_llama = 'llama' in args.model_name or 'facebook/opt' in args.model_name\n",
    "prepend_bos = not is_llama\n",
    "\n",
    "prefixes=args.prefixes\n",
    "separators=args.separators\n",
    "\n",
    "compute_ppl=False\n",
    "shuffle_labels=False\n",
    "generate_str=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset: antonym_len_8_1 with 1501 train, 194 valid and 450 test samples\n",
      "Example: 1) {'input': 'figured', 'output': 'plain'}\n",
      "Example: 2) {'input': 'bent', 'output': 'straight'}\n"
     ]
    }
   ],
   "source": [
    "dataset = exp_ra_utils.data_loading_wrapper(args.dataset_name, \n",
    "                                            root_data_dir=args.root_data_dir,\n",
    "                                            seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: historic\\nA: modern\\n\\nQ: figured\\nA: plain\\n\\nQ: bent\\nA: straight\\n\\nQ: convergence\\nA: divergence\\n\\nQ: more\\nA: less\\n\\nQ: lifelong\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: lifelong\\nA:'\n",
      "Input Sentence: '<|endoftext|>Q: historic\\nA: modern\\n\\nQ: figured\\nA: plain\\n\\nQ: bent\\nA: straight\\n\\nQ: convergence\\nA: divergence\\n\\nQ: more\\nA: less\\n\\nQ: lifelong\\nA:' \n",
      "\n",
      "Input Query: 'lifelong', Target: 'temporary'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' lifelong', 0.17095), (' lifetime', 0.12667), (' long', 0.04274), (' life', 0.02816), (' more', 0.02066)] \n",
      "\n",
      "Input Sentence: '<|endoftext|>Q: lifelong\\nA:' \n",
      "\n",
      "Input Query: 'lifelong', Target: 'temporary'\n",
      "\n",
      "Zero-Shot Prompt Top K Vocab Probs:\n",
      " [(' I', 0.1022), (' Yes', 0.03696), (' The', 0.02445), (' No', 0.02331), (' It', 0.01985)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_example = exp_ra_utils.data_print_and_test_example(model, tokenizer=tokenizer,\n",
    "                                                            dataset=dataset,\n",
    "                                                            prepend_bos=prepend_bos,\n",
    "                                                            prefixes=prefixes,\n",
    "                                                            separators=separators,\n",
    "                                                            shuffle_labels=shuffle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params to optimize: 24\n",
      "Going to edit only the following layers (showing only the first 10): ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.weight']\n",
      "Going to edit only the following layers (showing only the last 10): ['transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "params_names = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if params_names_filter(param_name):\n",
    "        param.requires_grad_(True)  # only relevant layer are trained (all the rest are freezed)\n",
    "        params.append(param)\n",
    "        params_names.append(param_name)\n",
    "print(f'Number of params to optimize: {len(params)}')\n",
    "print('Going to edit only the following layers (showing only the first 10):', params_names[:10])\n",
    "print('Going to edit only the following layers (showing only the last 10):', params_names[-10:])\n",
    "\n",
    "opt = torch.optim.SGD(params, lr=1e-3, weight_decay=0)  # learning rate does not really matter as we only intrest in the VJPs which we collect\n",
    "min_loss_for_update = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lm_config = [config.attn_o]  # the only type of layers we need to calculate the reverse attn (witht he attn values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_collector(example_index, split='train', annotate=False, dataset=dataset):\n",
    "    collector = {}\n",
    "    j = example_index\n",
    "    for curr_n_shots in n_shots_icl:\n",
    "        run_type = curr_n_shots\n",
    "        # hs_collector = llm_utils.wrap_model(model, layers_to_check=args.llm_config_path, \n",
    "        #                                     return_hooks_handler=True, forward=True, max_len=1000)\n",
    "        # grad_collector = llm_utils.wrap_model(model, layers_to_check=args.llm_config_path, \n",
    "        #                                     return_hooks_handler=True, forward=False, max_len=1000)\n",
    "        hs_collector = {}\n",
    "        grad_collector = llm_utils.wrap_model(model, layers_to_check=short_lm_config, \n",
    "                                            return_hooks_handler=True, forward=False, max_len=1000)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        if curr_n_shots == 0:\n",
    "            word_pairs = {'input':[], 'output':[]}\n",
    "        else:\n",
    "            random_samples_without_current_j = np.random.choice(len(dataset[split]), curr_n_shots, replace=False)\n",
    "            while j in random_samples_without_current_j:\n",
    "                random_samples_without_current_j = np.random.choice(len(dataset[split]), curr_n_shots, replace=False)\n",
    "\n",
    "            word_pairs = dataset[split][random_samples_without_current_j]\n",
    "        word_pairs_test = dataset[split][j]\n",
    "        if prefixes is not None and separators is not None:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                    shuffle_labels=shuffle_labels, prefixes=prefixes, separators=separators)\n",
    "        else:\n",
    "            prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                    shuffle_labels=shuffle_labels)\n",
    "            \n",
    "        # Get relevant parts of the Prompt\n",
    "        query, target = prompt_data['query_target']['input'], prompt_data['query_target']['output']\n",
    "        query = query[0] if isinstance(query, list) else query\n",
    "        if generate_str:\n",
    "            target = [target] if not isinstance(target, list) else target\n",
    "        else:\n",
    "            target = target[0] if isinstance(target, list) else target\n",
    "        \n",
    "        sentence = [create_prompt(prompt_data)]\n",
    "        \n",
    "        # # Figure out tokens of interest\n",
    "        # target_token_id = get_answer_id(sentence[0], target, tokenizer)\n",
    "    \n",
    "        device = model.device\n",
    "        inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "        original_pred_idx = len(inputs.input_ids.squeeze()) - 1\n",
    "\n",
    "        # if compute_nll:\n",
    "        target_completion = \"\".join(sentence + [target])\n",
    "        nll_inputs = tokenizer(target_completion, return_tensors='pt').to(device)\n",
    "        nll_targets = nll_inputs.input_ids.clone()\n",
    "        target_len = len(nll_targets.squeeze()) - len(inputs.input_ids.squeeze()) \n",
    "        nll_targets[:,:-target_len] = -100  # This is the accepted value to skip indices when computing loss in nn.CrossEntropyLoss\n",
    "\n",
    "        if annotate:\n",
    "            tmp_print = target_completion.replace(\"\\n\", \"\\\\n\")\n",
    "            print(f'[{run_type}-shots] len={nll_inputs.input_ids.shape[1]}, sentence_with_target=\"{tmp_print}\", only target=\"{target}\"')\n",
    "\n",
    "        output = model(**nll_inputs, labels=nll_targets, output_attentions=True, output_hidden_states=True, use_cache=True)\n",
    "        \n",
    "         # compute gradients but do not apply the step of the optimizer\n",
    "        # if clean_nll >= min_loss_for_update:\n",
    "        # clean_nll = output.loss.item()\n",
    "        # clean_output = output.logits[:,original_pred_idx,:]\n",
    "        output.loss.backward()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        collector[run_type] = {\n",
    "            # 'hs': copy.deepcopy(hs_collector),\n",
    "            'grad': copy.deepcopy(grad_collector),\n",
    "            'kv_cache': output.past_key_values,\n",
    "            'attentions': output.attentions,\n",
    "            # 'inputs': copy.deepcopy(nll_inputs)\n",
    "        }\n",
    "\n",
    "\n",
    "        llm_utils.remove_collector_hooks(hs_collector)\n",
    "        llm_utils.remove_collector_hooks(grad_collector)\n",
    "    return collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_index: 0/5\n",
      "[0-shots] len=8, sentence_with_target=\"<|endoftext|>Q: historic\\nA: modern\", only target=\" modern\"\n",
      "[5-shots] len=53, sentence_with_target=\"<|endoftext|>Q: destructive\\nA: constructive\\n\\nQ: elevation\\nA: depression\\n\\nQ: bodily\\nA: spiritual\\n\\nQ: disparate\\nA: similar\\n\\nQ: figured\\nA: plain\\n\\nQ: historic\\nA: modern\", only target=\" modern\"\n"
     ]
    }
   ],
   "source": [
    "all_rev_attns = {}\n",
    "all_forward_attns = {}\n",
    "\n",
    "for n_shots in n_shots_icl:\n",
    "    all_rev_attns[n_shots] = {}\n",
    "    all_forward_attns[n_shots] = {}\n",
    "    for layer_index in range(n_layer):\n",
    "        all_rev_attns[n_shots][layer_index] = {}\n",
    "        all_forward_attns[n_shots][layer_index] = {}\n",
    "        for head_index in range(n_head):\n",
    "            all_rev_attns[n_shots][layer_index][head_index] = []\n",
    "            all_forward_attns[n_shots][layer_index][head_index] = []\n",
    "    \n",
    "for example_index in range(n_samples):\n",
    "    if example_index % 5 == 0:\n",
    "        print(f'example_index: {example_index}/{n_samples}')\n",
    "    collector = get_experiment_collector(example_index=example_index, annotate= example_index%5 == 0)\n",
    "    for n_shots in n_shots_icl:\n",
    "        for layer_index in range(n_layer):\n",
    "            for head_index in range(n_head):\n",
    "                # forward_attn_map, rev_attn_map = get_forward_and_reversed_attn(collector[n_shots], layer_index, head_index)\n",
    "                forward_attn_map, rev_attn_map = exp_ra_utils.get_forward_and_reversed_attn(collector[n_shots], layer_index, head_index, config, head_size)\n",
    "                all_rev_attns[n_shots][layer_index][head_index].append(rev_attn_map)\n",
    "                all_forward_attns[n_shots][layer_index][head_index].append(forward_attn_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish averaging the reverse attentions for 5 samples\n",
      "The amount of reverse attentions for each head and layer is 5\n"
     ]
    }
   ],
   "source": [
    "reversed_attention_intervention = {}\n",
    "forward_attention_intervention = {}\n",
    "for n_shots in n_shots_icl:\n",
    "    reversed_attention_intervention[n_shots] = {}\n",
    "    forward_attention_intervention[n_shots] = {}\n",
    "    for layer_index in range(n_layer):\n",
    "        reversed_attention_intervention[n_shots][layer_index] = {}\n",
    "        forward_attention_intervention[n_shots][layer_index] = {}\n",
    "        for head_index in range(n_head):\n",
    "            # assuming all prompts are in the same length and format, hence we can average the Reverse Attentions maps\n",
    "            mean_rev_attn = sum(all_rev_attns[n_shots][layer_index][head_index]) / len(all_rev_attns[n_shots][layer_index][head_index])\n",
    "            reversed_attention_intervention[n_shots][layer_index][head_index] = mean_rev_attn.detach().clone().to(device)\n",
    "\n",
    "            mean_forward_attn = sum(all_forward_attns[n_shots][layer_index][head_index]) / len(all_forward_attns[n_shots][layer_index][head_index])\n",
    "            forward_attention_intervention[n_shots][layer_index][head_index] = mean_forward_attn.detach().clone().to(device)\n",
    "\n",
    "print(f'Finish averaging the reverse attentions for {n_samples} samples')\n",
    "print(f'The amount of reverse attentions for each head and layer is {len(all_rev_attns[n_shots_icl[0]][2][3])}')\n",
    "# sns.heatmap(reversed_attention_intervention['zsl'][layer_index][head_index].detach().cpu().numpy(), cmap='bwr')\n",
    "map_lens_to_n_shots = {}\n",
    "for n_shots in n_shots_icl:\n",
    "    curr_len = reversed_attention_intervention[n_shots][2][3].shape[0]\n",
    "    map_lens_to_n_shots[curr_len] = n_shots  # correct for reversed and forward attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How we edits the forward pass:\n",
    "We use a method known as function patching (sometime refer as \"monkey patching\") to replace the forward pass of the model with our own forward pass.\n",
    "\n",
    "We copy from transformers (the library by HuggingFace) the forward pass of the model, modify it, and then we replace the forward pass call of the model with our own version.\n",
    "\n",
    "In the copied forward pass we add a our new code, which involve dynamicly adding an attention maps to the forward pass' attention maps.\n",
    "\n",
    "our code is marked with #### (all the rest of the code is from transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aux_func_opt(self, layer_index, lr, intervention_with_reversed_attention=True):\n",
    "    def forward(\n",
    "        # self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        key_value_states: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        bsz, tgt_len, _ = hidden_states.size()\n",
    "\n",
    "        # get query proj\n",
    "        query_states = self.q_proj(hidden_states) * self.scaling\n",
    "        # get key, value proj\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_states = past_key_value[0]\n",
    "            value_states = past_key_value[1]\n",
    "        elif is_cross_attention:\n",
    "            # cross_attentions\n",
    "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
    "        elif past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "        else:\n",
    "            # self_attention\n",
    "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
    "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_states, value_states)\n",
    "\n",
    "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
    "        key_states = key_states.view(*proj_shape)\n",
    "        value_states = value_states.view(*proj_shape)\n",
    "\n",
    "        src_len = key_states.size(1)\n",
    "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
    "\n",
    "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            attn_weights = torch.max(\n",
    "                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n",
    "            )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n",
    "        if attn_weights.dtype == torch.float16:\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n",
    "        else:\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        if layer_head_mask is not None:\n",
    "            if layer_head_mask.size() != (self.num_heads,):\n",
    "                raise ValueError(\n",
    "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n",
    "                    f\" {layer_head_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        if output_attentions:\n",
    "            # this operation is a bit awkward, but it's required to\n",
    "            # make sure that attn_weights keeps its gradient.\n",
    "            # In order to do so, attn_weights have to be reshaped\n",
    "            # twice and have to be reused in the following\n",
    "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "        ################# our trick\n",
    "        if len(attn_weights.shape) != 3: # assuming [n_head, seq_len, seq_len]\n",
    "            print(f'Warning: len(attn_weights.shape) != 3: attn_weights.shape: {attn_weights.shape}')\n",
    "        \n",
    "        for head_index in range(n_head):\n",
    "            # forward_attn_map, rev_attn_map = get_forward_and_reversed_attn(collector, layer_index, head_index)\n",
    "            # attn_weights[0, head_index] += lr * rev_attn_map\n",
    "            if attn_weights.shape[2] not in map_lens_to_n_shots:\n",
    "                print(f'Error: could not find intervention for layer_index={layer_index}, head_index={head_index}')\n",
    "                print(f'attn_weights.shape: {attn_weights.shape}')\n",
    "                raise ValueError(f'could not find intervention for layer_index={layer_index}, head_index={head_index}')\n",
    "            curr_n_shots = map_lens_to_n_shots[attn_weights.shape[2]]\n",
    "            if intervention_with_reversed_attention:\n",
    "                intervention_map = reversed_attention_intervention[curr_n_shots][layer_index][head_index]\n",
    "            else:\n",
    "                intervention_map = forward_attention_intervention[curr_n_shots][layer_index][head_index]\n",
    "            attn_weights[head_index] += lr * intervention_map\n",
    "        #################\n",
    "\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "    \n",
    "        attn_output = torch.bmm(attn_probs, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n",
    "        # partitioned aross GPUs when using tensor-parallelism.\n",
    "        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value\n",
    "    \n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aux_func_gpt2(self, layer_index, lr, intervention_with_reversed_attention=True):\n",
    "    # print(f'Connecting to layer_index={layer_index}, lr={lr}')\n",
    "    def _attn(\n",
    "            # self, \n",
    "              query, key, value, attention_mask=None, head_mask=None):\n",
    "            attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "\n",
    "            if self.scale_attn_weights:\n",
    "                attn_weights = attn_weights / torch.full(\n",
    "                    [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
    "                )\n",
    "\n",
    "            # Layer-wise attention scaling\n",
    "            if self.scale_attn_by_inverse_layer_idx:\n",
    "                attn_weights = attn_weights / float(self.layer_idx + 1)\n",
    "\n",
    "            if not self.is_cross_attention:\n",
    "                # if only \"normal\" attention layer implements causal mask\n",
    "                query_length, key_length = query.size(-2), key.size(-2)\n",
    "                causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
    "                mask_value = torch.finfo(attn_weights.dtype).min\n",
    "                # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
    "                # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
    "                mask_value = torch.full([], mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "                attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                # Apply the attention mask\n",
    "                attn_weights = attn_weights + attention_mask\n",
    "\n",
    "            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "            # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
    "            attn_weights = attn_weights.type(value.dtype)\n",
    "            attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "            # Mask heads if we want to\n",
    "            if head_mask is not None:\n",
    "                attn_weights = attn_weights * head_mask\n",
    "\n",
    "            ################# our trick\n",
    "            if attn_weights.shape[0] != 1:\n",
    "                print(f'Warning: attn_weights.shape[0] != 1: attn_weights.shape: {attn_weights.shape}')\n",
    "                \n",
    "            for head_index in range(n_head):\n",
    "                if attn_weights.shape[3] not in map_lens_to_n_shots:\n",
    "                    print(f'Error: could not find intervention for layer_index={layer_index}, head_index={head_index}')\n",
    "                    print(f'attn_weights.shape: {attn_weights.shape}')\n",
    "                    raise ValueError(f'could not find intervention for layer_index={layer_index}, head_index={head_index}')\n",
    "                curr_n_shots = map_lens_to_n_shots[attn_weights.shape[3]]\n",
    "                if intervention_with_reversed_attention:\n",
    "                    intervention_map = reversed_attention_intervention[curr_n_shots][layer_index][head_index]\n",
    "                else:\n",
    "                    intervention_map = forward_attention_intervention[curr_n_shots][layer_index][head_index]\n",
    "                attn_weights[0, head_index] += lr * intervention_map\n",
    "            #################\n",
    "\n",
    "            attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "            return attn_output, attn_weights\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            # self,\n",
    "            hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "            layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            use_cache: Optional[bool] = False,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "        ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "            if encoder_hidden_states is not None:\n",
    "                if not hasattr(self, \"q_attn\"):\n",
    "                    raise ValueError(\n",
    "                        \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
    "                        \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n",
    "                    )\n",
    "\n",
    "                query = self.q_attn(hidden_states)\n",
    "                key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
    "                attention_mask = encoder_attention_mask\n",
    "            else:\n",
    "                query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "\n",
    "            query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "            key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "            value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "            if layer_past is not None:\n",
    "                past_key, past_value = layer_past\n",
    "                key = torch.cat((past_key, key), dim=-2)\n",
    "                value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "            if use_cache is True:\n",
    "                present = (key, value)\n",
    "            else:\n",
    "                present = None\n",
    "\n",
    "            if self.reorder_and_upcast_attn:\n",
    "                attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "            else:\n",
    "                attn_output, attn_weights = _attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "            attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "            attn_output = self.c_proj(attn_output)\n",
    "            attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "            outputs = (attn_output, present)\n",
    "            if output_attentions:\n",
    "                outputs += (attn_weights,)\n",
    "\n",
    "            return outputs  # a, present, (attentions)\n",
    "\n",
    "    return forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_to_examine = [3, 2, 1, 0.5, 0, -0.1, 0.25, -0.5, -1] + np.arange(-2, -10, -2).tolist() + np.arange(-10, -40, -5).tolist() + np.arange(-40, -121, -20).tolist()\n",
    "if DEBUG_FLAG:\n",
    "    lr_to_examine = [3, 0, -3, -15]\n",
    "\n",
    "columns = ['n_shots', 'lr', 'top_1', 'top_2', 'top_3']\n",
    "\n",
    "df_rev_path_out = f'{args.out_folder}/{output_prefix}_intervention_via_reverse_attn.csv'\n",
    "df_forward_path_out = f'{args.out_folder}/{output_prefix}_intervention_via_forward_attn.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {\n",
    "             'df_rev_path_out': df_rev_path_out,\n",
    "             'df_forward_path_out': df_forward_path_out,\n",
    "             'example_sentence': sentence_example,  # to verify we ran what we wanted\n",
    "             'ds_train_len': len(dataset['train']),\n",
    "             'ds_valid_len': len(dataset['valid']),\n",
    "             'ds_test_len': len(dataset['test']),\n",
    "             'range_of_percentages': lr_to_examine,\n",
    "             'run_args': str(args),\n",
    "             'debug_flag_on': DEBUG_FLAG}\n",
    "meta_data_out = os.path.join(args.out_folder, f'{output_prefix}_exp_intervention_meta_data.json')\n",
    "if args.check_if_result_already_exists and os.path.exists(meta_data_out):\n",
    "    print(f'File already exists: {meta_data_out}. Terminating this run. If you wish to re-run, please remove the file or disable the flag --check_if_result_already_exists')\n",
    "    print(f'Run args: {args}')\n",
    "    print('Exit...')\n",
    "    exit(0)\n",
    "\n",
    "with open(meta_data_out, 'w') as f:\n",
    "    json.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = model.to('cpu')  # not really needed, but just to be sure\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# re-initialize the model just to make sure we used one that was not fine-tuned or altered in any way\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_extra_args).eval().requires_grad_(False).to(device)\n",
    "\n",
    "for df_path, rev_or_forward in [(df_rev_path_out, True), (df_forward_path_out, False)]:\n",
    "    df = pd.DataFrame(data=[], columns=columns)\n",
    "\n",
    "    print(f'Start intervention via {\"reverse\" if rev_or_forward else \"forward\"} attention...')\n",
    "    # change all attn layers forward pass function to take the intervention on the attention maps\n",
    "    # notice we do not need to reinitialize the model, as we overwrite the forward function at each iteration of the lr loop (no leakings between iterations)\n",
    "    for lr_index, lr in enumerate(lr_to_examine):\n",
    "        print(f'\\n******** lr: {lr} ********')\n",
    "        print('Connecting to layer', end='')\n",
    "        for layer_index in range(n_layer):\n",
    "            print(f' {layer_index}', end='')\n",
    "            attn_layer = llm_utils.rgetattr(model, config.layer_attn_format.format(layer_index))\n",
    "            if 'gpt2' in args.model_name.lower():\n",
    "                attn_layer.forward = create_aux_func_gpt2(attn_layer, layer_index=layer_index, \n",
    "                                                          lr=lr, intervention_with_reversed_attention=rev_or_forward)\n",
    "            elif 'opt' in args.model_name.lower():\n",
    "                attn_layer.forward = create_aux_func_opt(attn_layer, layer_index=layer_index,\n",
    "                                                         lr=lr, intervention_with_reversed_attention=rev_or_forward)\n",
    "            else:\n",
    "                print(f'Error: could not find model type for {args.model_name}')\n",
    "                raise ValueError(f'could not find model type for {args.model_name}')\n",
    "        print(f' --> all layer are ready!')\n",
    "        # first, evaluate the model on the ICL task\n",
    "        print(f\"ICL Results:\")\n",
    "        # curr_res_icl = aux_eval_model(model, n_shots_list=n_shots_icl, annotate=lr_index % 5 == 0)\n",
    "        curr_res_icl = exp_ra_utils.aux_eval_model(model, tokenizer=tokenizer,\n",
    "                                                   model_name=args.model_name,\n",
    "                                                   dataset=dataset,\n",
    "                                                   metric_to_eval=metric_to_use,\n",
    "                                                   n_shots_list=n_shots_icl,\n",
    "                                                   prefixes=prefixes, separators=separators,\n",
    "                                                   compute_ppl=True,\n",
    "                                                   annotate=lr_index % 5 == 0)\n",
    "        # print(f\"ICL Results:\")\n",
    "        # print(res_icl)\n",
    "        for n_shots, res in curr_res_icl.items():\n",
    "            df.loc[len(df)] = [n_shots, lr, res['clean_topk'][0][1], res['clean_topk'][1][1], res['clean_topk'][2][1]]\n",
    "\n",
    "        if lr_index % 5 == 0:\n",
    "            print('Backup saving...')\n",
    "            df.to_csv(df_path, index=False)\n",
    "\n",
    "    df.to_csv(df_path, index=False)\n",
    "    print(f'Finish intervention via {\"reverse\" if rev_or_forward else \"forward\"} attention. Info saved at {df_path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
