{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# another code we wrote\n",
    "sys.path.append('../')\n",
    "import llm_utils\n",
    "import opt_utils\n",
    "from plot_utils import plot_aux_wrapper\n",
    "import exp_ra_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     from function_vectors.src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "#     from function_vectors.src.utils.eval_utils import decode_to_vocab, sentence_eval, n_shot_eval_no_intervention, get_answer_id\n",
    "# except Exception as error:\n",
    "#     print('could not import from function_vectors package with the following error:')\n",
    "#     print(error)\n",
    "#     print('Make sure you first pull relevant submodules. See README.md for more info.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "# DEBUG_FLAG = True  # for local testing\n",
    "DEBUG_FLAG = False  # automatically set below\n",
    "\n",
    "try:\n",
    "    DEBUG_FLAG = torch.backends.mps.is_available()  # since only Apple M1-3 supports this, we can use this as a flag for local testing\n",
    "except:\n",
    "    pass\n",
    "if DEBUG_FLAG:\n",
    "    print('*'*40)\n",
    "    print(f'    DEBUG MODE [{START_TIME}]')\n",
    "    print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='gpt2' if DEBUG_FLAG else 'gpt2-xl')\n",
    "parser.add_argument('--model_args', type=str, default='')\n",
    "parser.add_argument('--out_folder', type=str, default='/tmp_plots1')\n",
    "parser.add_argument('--postfix_name', type=str, default='')\n",
    "parser.add_argument('--disable_pad_token', action='store_true')\n",
    "parser.add_argument('--root_data_dir', type=str, default='../function_vectors/dataset_files')\n",
    "parser.add_argument('--dataset_name', type=str, default='person-sport')\n",
    "parser.add_argument('--heads_list_path', type=str, default='index')\n",
    "parser.add_argument('--n_shots_icl', type=str, default='0,5' if DEBUG_FLAG else '0,1,5,10')\n",
    "parser.add_argument('--n_samples', type=int, default=5 if DEBUG_FLAG else 25)\n",
    "parser.add_argument('--percentage_step', type=float, default=0.333 if DEBUG_FLAG else 0.1)\n",
    "parser.add_argument('--metric_to_use', type=str, default='f1_score')  # f1_score, exact_match_score, first_word_score. Not really used\n",
    "parser.add_argument('--prefixes', help='Prompt template prefixes to be used', type=json.loads, required=False, default={\"input\":\"Q:\", \"output\":\"A:\", \"instructions\":\"\"})\n",
    "parser.add_argument('--separators', help='Prompt template separators to be used', type=json.loads, required=False, default={\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"})    \n",
    "parser.add_argument('--device', type=str, default=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--check_if_result_already_exists', action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown = parser.parse_known_args()\n",
    "print('unknown args:', unknown)\n",
    "print('args:', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots_icl = [int(x) for x in args.n_shots_icl.split(',')]\n",
    "print(f'n_shots_icl: {n_shots_icl}')\n",
    "metric_to_use = args.metric_to_use\n",
    "n_samples = args.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prefix is a code for which version of the code we use in this notebook\n",
    "output_prefix = f'per1_{args.model_name.replace(\"/\", \"-\")}_{args.dataset_name}_[{args.n_shots_icl}]_{n_samples}_{args.postfix_name}_'\n",
    "\n",
    "print(f'output_prefix: {output_prefix}')\n",
    "plot_wrapper = plot_aux_wrapper(output_folder=args.out_folder, \n",
    "                            output_prefix=output_prefix,\n",
    "                            local_show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "if not args.disable_pad_token:\n",
    "    print(f'adding pad token: {tokenizer.eos_token}')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "try:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # not blocking, just to prevent warnings and faster tokenization\n",
    "except:\n",
    "    pass\n",
    "padding_flag = tokenizer.pad_token_id is not None  # this is up to if the tokenizer was configured with padding or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "print(f'Using device: {device} [cuda available? => {torch.cuda.is_available()}, cuda version: {torch.version.cuda}, args.device = \"{args.device}\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extra_args = {}\n",
    "for arg in args.model_args.split(','):\n",
    "    if arg == '':\n",
    "        continue\n",
    "    k, v = arg.split('=')\n",
    "    model_extra_args[k] = v\n",
    "print(f'model_extra_args: {model_extra_args}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_extra_args).eval().requires_grad_(False).to(device)\n",
    "model_aux = llm_utils.model_extra(model=model, device=device)\n",
    "model_config = copy.deepcopy(model.config)\n",
    "config = model_aux.config\n",
    "# del model\n",
    "\n",
    "n_embd = model_aux.n_embd\n",
    "n_head = model_aux.n_head\n",
    "head_size = model_aux.head_size\n",
    "n_layer = model_aux.n_layer\n",
    "\n",
    "pad_k = model_aux.pad_k\n",
    "pad_v = model_aux.pad_v\n",
    "\n",
    "# params_names_filter = opt_utils.get_filter_by_name(args.filter_layers)\n",
    "params_names_filter = opt_utils.get_filter_by_name('attn_only')\n",
    "# params_names_filter = lambda x: ('attn' in x or 'attention' in x) and 'weight' in x\n",
    "\n",
    "show_every_n_layer = 1 if n_layer <= 12 else 2 if n_layer <= 24 else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_llama = 'llama' in args.model_name or 'facebook/opt' in args.model_name  # llama and opt have similar configs and tokenization\n",
    "prepend_bos = not is_llama\n",
    "\n",
    "prefixes=args.prefixes\n",
    "separators=args.separators\n",
    "\n",
    "compute_ppl=False\n",
    "shuffle_labels=False\n",
    "generate_str=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = exp_ra_utils.data_loading_wrapper(args.dataset_name, \n",
    "                                            root_data_dir=args.root_data_dir,\n",
    "                                            seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_example = exp_ra_utils.data_print_and_test_example(model, tokenizer=tokenizer,\n",
    "                                                            dataset=dataset,\n",
    "                                                            prepend_bos=prepend_bos,\n",
    "                                                            prefixes=prefixes,\n",
    "                                                            separators=separators,\n",
    "                                                            shuffle_labels=shuffle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = model.to('cpu')  # extra safety for the following experiments\n",
    "    del model\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alter the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_perturbation_ablation(percentage_of_heads, sort_list_of_heads):\n",
    "    # explicitly create and delte the model to make sure we used the right config\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_extra_args).eval().requires_grad_(False).to(device)\n",
    "    heads_to_use = sort_list_of_heads[:int(len(sort_list_of_heads)*percentage_of_heads)]\n",
    "    print(f'going to use {len(heads_to_use)} heads for {percentage_of_heads}')\n",
    "    print(f'first 3 heads: {heads_to_use[:5]}')\n",
    "    print(f'last 3 heads: {heads_to_use[-5:]}')\n",
    "    used_heads = {f'{x[0]}_{x[1]}': False for x in heads_to_use}\n",
    "    for layer_index in range(n_layer):\n",
    "        Wo = llm_utils.rgetattr(model, config.attn_o.format(layer_index))\n",
    "        for head_index in range(n_head):\n",
    "            if f'{layer_index}_{head_index}' in used_heads:\n",
    "                # print(f'{layer_index}_{head_index}')\n",
    "                # if this head should be used - do not change it\n",
    "                used_heads[f'{layer_index}_{head_index}'] = True  # just an indicator for debugging\n",
    "            else: # if we do not want to use this head - zero it (equal to block its output)\n",
    "                if type(Wo) == Conv1D:\n",
    "                    Wo.weight[head_size*head_index:head_size*(head_index+1)] = 0\n",
    "                elif type(Wo) == nn.Linear:\n",
    "                    Wo.weight[:, head_size*head_index:head_size*(head_index+1)] = 0\n",
    "                else:\n",
    "                    raise ValueError(f'Unknown type of Wo: {type(Wo)} (currently only transformers.pytorch_utils.Conv1D and torch.nn.Linear are supported)')\n",
    "    \n",
    "    # res_icl = aux_eval_model(model, n_shots_list=n_shots_icl)\n",
    "    res_icl = exp_ra_utils.aux_eval_model(model, tokenizer=tokenizer,\n",
    "                                          model_name=args.model_name,\n",
    "                                          dataset=dataset,\n",
    "                                          metric_to_eval=metric_to_use,\n",
    "                                          n_shots_list=n_shots_icl,\n",
    "                                          prefixes=prefixes, separators=separators,\n",
    "                                          compute_ppl=True,\n",
    "                                          annotate=False)\n",
    "\n",
    "    model = model.to('cpu')  # extra safety (not really needed)\n",
    "    del model\n",
    "\n",
    "    return res_icl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_used = None\n",
    "nshot_that_used_loaded_list = 777\n",
    "if args.heads_list_path == 'rand':\n",
    "    print(f'Going to randomize the heads list')\n",
    "    method_used = 'rand'\n",
    "    all_heads = [(layer_index, head_index, random.random()) for layer_index in range(n_layer) for head_index in range(n_head)]\n",
    "elif args.heads_list_path == 'index':\n",
    "    print(f'Going to use all heads by the order of their index (layer, head)')\n",
    "    method_used = 'index'\n",
    "    all_heads = [(layer_index, head_index, 0) for layer_index in range(n_layer) for head_index in range(n_head)]\n",
    "elif 'attn_maps_norms_mean.csv' in args.heads_list_path:  # assuming backward or forward attention maps (mean across examples)\n",
    "    print(f'\\nGoing to load the heads list from {args.heads_list_path}')\n",
    "    method_used = 'backward' if 'backward' in args.heads_list_path else 'forward'\n",
    "    method_used = 'reverse' if 'reverse' in args.heads_list_path else method_used\n",
    "    nshot_that_used_loaded_list = args.heads_list_path.split('[')[-1].split(']')[0]\n",
    "    tmp = pd.read_csv(args.heads_list_path)\n",
    "    all_heads = []\n",
    "    for col in tmp.columns:\n",
    "        if 'layer_' not in col:\n",
    "            print(f'While processing the csv heads list file encounter the following column and skipped it: {col}')\n",
    "            continue\n",
    "        layer_index = int(col.split('_')[1])\n",
    "        for row_index, row in tmp.iterrows():\n",
    "            all_heads.append((layer_index, row_index, row[col]))\n",
    "elif 'simple_casual_mediation' in args.heads_list_path:  # assuming simple casual mediation (our implementation)\n",
    "    print(f'Loading heads list from {args.heads_list_path} (assuming simple casual mediation)')\n",
    "    tmp = torch.load(args.heads_list_path)\n",
    "    print(f'heads list info is in shape of {tmp.shape}')\n",
    "    if len(tmp.shape) == 3:\n",
    "        print(f'Assuming the [n_examples, n_layer, n_head] shape. averaging over n_examples')\n",
    "        tmp = tmp.mean(dim=0)  # in case (n_examples, n_layer, n_head) that was used in previous versions (currenttly should be (n_layer, n_head))\n",
    "\n",
    "    method_used = 'simple_causal_mediation'\n",
    "    # now tmp should be of shape (n_layer, n_head). at every layer, head, we have the average indirect effect (AIE)\n",
    "    print(f'Reduce heads list info to shape: {tmp.shape}. Assuming the [layer, head] entry holds the average indirect effect for that attention head')\n",
    "    assert tmp.shape == (n_layer, n_head)\n",
    "    nshot_that_used_loaded_list = args.heads_list_path.split('[')[-1].split(']')[0]\n",
    "    all_heads = []\n",
    "    for layer_index in range(n_layer):\n",
    "        for head_index in range(n_head):\n",
    "            all_heads.append((layer_index, head_index, tmp[layer_index, head_index].item()))\n",
    "elif '_indirect_effect.pt' in args.heads_list_path:  # assuming Function Vectors implementation of indirect effect as tensor\n",
    "    print(f'Loading heads list from {args.heads_list_path} (assuming indirect effect pt)')\n",
    "    nshot_that_used_loaded_list = args.heads_list_path.split('LTO')[-1].split('AT')[-1].split('/')[0]\n",
    "    tmp = torch.load(args.heads_list_path)\n",
    "    print(f'heads list info is in shape of {tmp.shape}')\n",
    "    if len(tmp.shape) == 4:  #  causal indirect effect (CIE) was taken for all tokens (if not - we assume it is for the last token only)\n",
    "        tmp = tmp.mean(dim=-1)\n",
    "        method_used = 'indirect_effect_AT'\n",
    "    else:\n",
    "        method_used = 'indirect_effect_LTO'\n",
    "\n",
    "    # now tmp should be of shape (n_examples, n_layer, n_head). at every example, layer, head, we have the CIE\n",
    "    tmp = tmp.mean(dim=0)\n",
    "    # now tmp should be of shape (n_layer, n_head). at every layer, head, we have the average indirect effect (AIE)\n",
    "    print(f'Reduce heads list info to shape: {tmp.shape}. Assuming the [layer, head] entry holds the average indirect effect for that attention head')\n",
    "    assert tmp.shape == (n_layer, n_head)\n",
    "    all_heads = []\n",
    "    for layer_index in range(n_layer):\n",
    "        for head_index in range(n_head):\n",
    "            all_heads.append((layer_index, head_index, tmp[layer_index, head_index].item()))\n",
    "else:\n",
    "    raise ValueError(f'Unknown heads list format: {args.heads_list_path}')\n",
    "\n",
    "\n",
    "print(f'Assuming the following method was used to create the heads list: {method_used}, {nshot_that_used_loaded_list}')\n",
    "all_heads = sorted(all_heads, key=lambda x: x[2])\n",
    "all_heads.reverse()  # sort in descending order (assuming the higher the value the more important the head is)\n",
    "# please notice that after examining the orderer list, we also examin the reversed list (to make sure we did not miss anything)\n",
    "# to verify the order (and what is it) - we save an extra file with the order of the heads (meta_data.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_path_code = f'{output_prefix}_~ME:{method_used}~NS:{nshot_that_used_loaded_list}~'\n",
    "df_path_out = os.path.join(args.out_folder, f'{explicit_path_code}_exp_perturbation_ablation.csv')\n",
    "columns = ['percentage_of_heads']\n",
    "for n_shots in n_shots_icl:\n",
    "    columns.extend([f'{n_shots}_top_1', f'{n_shots}_top_2', f'{n_shots}_top_3'])\n",
    "df = pd.DataFrame(data=[], columns=columns)\n",
    "print(f'Going to save results to: {df_path_out}')\n",
    "if args.check_if_result_already_exists and os.path.exists(df_path_out):\n",
    "    print(f'File already exists: {df_path_out}. Terminating this run. If you wish to re-run, please remove the file or disable the flag --check_if_result_already_exists')\n",
    "    print(f'Run args: {args}')\n",
    "    print('Exit...')\n",
    "    exit(0)\n",
    "df.to_csv(df_path_out)  # to verify the file can be saved (currently empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_of_percentages = np.arange(0, 1.01, args.percentage_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'first 5 heads: {all_heads[:5]}')\n",
    "print(f'last 5 heads: {all_heads[-5:]}')\n",
    "meta_data = {'heads_order': all_heads, # the heads order that is saved in the meta data is before the reverse\n",
    "             'heads_source': args.heads_list_path,\n",
    "             'example_sentence': sentence_example,  # to verify we ran what we wanted\n",
    "             'df_path_out': df_path_out,\n",
    "             'range_of_percentages': [round(x, 2) for x in range_of_percentages],\n",
    "             'run_args': str(args),\n",
    "            'method_used': method_used,\n",
    "            'nshot_that_used_loaded_list': nshot_that_used_loaded_list,\n",
    "             'debug_flag_on': DEBUG_FLAG}\n",
    "meta_data_out = os.path.join(args.out_folder, f'{explicit_path_code}_exp_perturbation_ablation_meta_data.json')\n",
    "with open(meta_data_out, 'w') as f:\n",
    "    json.dump(meta_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_heads) == n_layer*n_head\n",
    "\n",
    "all_results = {}\n",
    "for percentage_of_heads in range_of_percentages:\n",
    "    percentage_of_heads = round(percentage_of_heads, 2)  # to handle floating point errors\n",
    "    res_icl = exp_perturbation_ablation(percentage_of_heads=percentage_of_heads, sort_list_of_heads=all_heads)\n",
    "    \n",
    "    for tmp_ in res_icl:\n",
    "        del res_icl[tmp_]['clean_rank_list']  # examine the top-{1,2,3} accuracy (compute_top_k_accuracy)\n",
    "        del res_icl[tmp_]['clean_ppl']\n",
    "    all_results[percentage_of_heads] = res_icl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percentage_of_heads, res in all_results.items():\n",
    "    row = [percentage_of_heads]\n",
    "    for n_shots in n_shots_icl:\n",
    "        row.extend([res[n_shots]['clean_topk'][0][1], res[n_shots]['clean_topk'][1][1], res[n_shots]['clean_topk'][2][1]])\n",
    "    df.loc[len(df)] = row\n",
    "df.to_csv(df_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finish with the experiment. Moving to apply the same process with the reversed heads list')\n",
    "all_heads_rev = all_heads.copy()\n",
    "all_heads_rev.reverse()\n",
    "print(f'now the first 10 reversed heads: {all_heads_rev[:10]}')\n",
    "print(f'now the last 10 reversed heads: {all_heads_rev[-10:]}')\n",
    "assert len(all_heads_rev) == n_layer*n_head\n",
    "\n",
    "all_results_rev = {}\n",
    "for percentage_of_heads in range_of_percentages:\n",
    "    percentage_of_heads = round(percentage_of_heads, 2)  # to handle floating point errors\n",
    "    # print(f'percentage_of_heads: {percentage_of_heads}')\n",
    "    res_icl = exp_perturbation_ablation(percentage_of_heads=percentage_of_heads, sort_list_of_heads=all_heads_rev)\n",
    "    for tmp_ in res_icl:\n",
    "        del res_icl[tmp_]['clean_rank_list']\n",
    "        del res_icl[tmp_]['clean_ppl']\n",
    "    all_results_rev[percentage_of_heads] = res_icl\n",
    "\n",
    "df_path_out_rev = df_path_out.replace('.csv', '_REV.csv')\n",
    "print(f'Going to save REVERSED ORDER results to: {df_path_out_rev}')\n",
    "df_rev = pd.DataFrame(data=[], columns=columns)\n",
    "for percentage_of_heads, res in all_results_rev.items():\n",
    "    row = [percentage_of_heads]\n",
    "    for n_shots in n_shots_icl:\n",
    "        row.extend([res[n_shots]['clean_topk'][0][1], res[n_shots]['clean_topk'][1][1], res[n_shots]['clean_topk'][2][1]])\n",
    "    df_rev.loc[len(df_rev)] = row\n",
    "df_rev.to_csv(df_path_out_rev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
