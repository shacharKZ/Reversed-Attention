{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "simple ablation study\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ks/miniconda3/envs/april24/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_utils.py: sys.path: ['/Users/ks/Documents/research/projD2/opt07/bl2/clean_bu_code_v2', '/Users/ks/miniconda3/envs/april24/lib/python310.zip', '/Users/ks/miniconda3/envs/april24/lib/python3.10', '/Users/ks/miniconda3/envs/april24/lib/python3.10/lib-dynload', '', '/Users/ks/miniconda3/envs/april24/lib/python3.10/site-packages', '/Users/ks/Documents/research/projD2/opt07/bl2/clean_bu_code_v2', '/Users/ks/Documents/research/projD2/opt07/bl2']\n",
      "llm_utils.py: sys.path: ['/Users/ks/Documents/research/projD2/opt07/bl2/clean_bu_code_v2', '/Users/ks/miniconda3/envs/april24/lib/python310.zip', '/Users/ks/miniconda3/envs/april24/lib/python3.10', '/Users/ks/miniconda3/envs/april24/lib/python3.10/lib-dynload', '', '/Users/ks/miniconda3/envs/april24/lib/python3.10/site-packages', '/Users/ks/Documents/research/projD2/opt07/bl2/clean_bu_code_v2', '/Users/ks/Documents/research/projD2/opt07/bl2', '/Users/ks/Documents/research/projD2/opt07/bl2']\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ks/miniconda3/envs/april24/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.pytorch_utils import Conv1D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "\n",
    "# another code we wrote\n",
    "sys.path.append('../')\n",
    "import llm_utils\n",
    "import opt_utils\n",
    "from plot_utils import plot_aux_wrapper\n",
    "import exp_ra_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from function_vectors.src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "    from function_vectors.src.utils.eval_utils import decode_to_vocab, sentence_eval, n_shot_eval_no_intervention, get_answer_id\n",
    "except Exception as error:\n",
    "    print('could not import from function_vectors package with the following error:')\n",
    "    print(error)\n",
    "    print('Make sure you first pull relevant submodules. See README.md for more info.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "    DEBUG MODE [2024/09/28-17:11:12]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "START_TIME = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "# DEBUG_FLAG = True  # for local testing\n",
    "DEBUG_FLAG = False  # automatically set below\n",
    "\n",
    "try:\n",
    "    DEBUG_FLAG = torch.backends.mps.is_available()  # since only Apple M1-3 supports this, we can use this as a flag for local testing\n",
    "except:\n",
    "    pass\n",
    "if DEBUG_FLAG:\n",
    "    print('*'*40)\n",
    "    print(f'    DEBUG MODE [{START_TIME}]')\n",
    "    print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--check_if_result_already_exists'], dest='check_if_result_already_exists', nargs=0, const=True, default=False, type=None, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model_name', type=str, default='facebook/opt-350m')\n",
    "parser.add_argument('--model_name', type=str, default='gpt2' if DEBUG_FLAG else 'gpt2-xl')\n",
    "parser.add_argument('--model_args', type=str, default='')\n",
    "parser.add_argument('--out_folder', type=str, default='/Users/ks/Documents/research/projD2/tmp_plots1')\n",
    "parser.add_argument('--postfix_name', type=str, default='')\n",
    "parser.add_argument('--disable_pad_token', action='store_true')\n",
    "parser.add_argument('--root_data_dir', type=str, default='/Users/ks/Documents/research/projD2/function_vectors/dataset_files' if DEBUG_FLAG else 'function_vectors_v2/dataset_files')\n",
    "# parser.add_argument('--dataset_name', type=str, default='antonym_len_8_1')\n",
    "parser.add_argument('--dataset_name', type=str, default='prev_item')\n",
    "parser.add_argument('--n_shots_icl', type=int, default=3)\n",
    "parser.add_argument('--n_samples', type=int, default=5 if DEBUG_FLAG else 25)\n",
    "parser.add_argument('--metric_to_use', type=str, default='f1_score')  # f1_score, exact_match_score, first_word_score\n",
    "parser.add_argument('--prefixes', help='Prompt template prefixes to be used', type=json.loads, required=False, default={\"input\":\"Q:\", \"output\":\"A:\", \"instructions\":\"\"})\n",
    "parser.add_argument('--separators', help='Prompt template separators to be used', type=json.loads, required=False, default={\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"})    \n",
    "parser.add_argument('--device', type=str, default=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--check_if_result_already_exists', action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown args: ['--f=/Users/ks/Library/Jupyter/runtime/kernel-v2-58089lRDM6XES3E6R.json']\n",
      "args: Namespace(model_name='gpt2', model_args='', out_folder='/Users/ks/Documents/research/projD2/tmp_plots1', postfix_name='', disable_pad_token=False, root_data_dir='/Users/ks/Documents/research/projD2/function_vectors/dataset_files', dataset_name='prev_item', n_shots_icl=3, n_samples=5, metric_to_use='f1_score', prefixes={'input': 'Q:', 'output': 'A:', 'instructions': ''}, separators={'input': '\\n', 'output': '\\n\\n', 'instructions': ''}, device=device(type='cpu'), seed=42, check_if_result_already_exists=False)\n"
     ]
    }
   ],
   "source": [
    "args, unknown = parser.parse_known_args()\n",
    "print('unknown args:', unknown)\n",
    "print('args:', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "if not args.disable_pad_token:\n",
    "    print(f'adding pad token: {tokenizer.eos_token}')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "try:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # not blocking, just to prevent warnings and faster tokenization\n",
    "except:\n",
    "    pass\n",
    "padding_flag = tokenizer.pad_token_id is not None  # this is up to if the tokenizer was configured with padding or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu [cuda available? => False, cuda version: None, args.device = \"cpu\"]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "print(f'Using device: {device} [cuda available? => {torch.cuda.is_available()}, cuda version: {torch.version.cuda}, args.device = \"{args.device}\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_extra_args: {}\n",
      "Loading config from /Users/ks/Documents/research/projD2/opt07/bl2/clean_bu_code_v2/flow_graph_configs/config_gpt2.json\n",
      "{'config_name': 'gpt2', 'layer_format': 'transformer.h.{}', 'layer_mlp_format': 'transformer.h.{}.mlp', 'layer_attn_format': 'transformer.h.{}.attn', 'ln1': 'transformer.h.{}.ln_1', 'attn_q': 'transformer.h.{}.attn.c_attn', 'attn_k': 'transformer.h.{}.attn.c_attn', 'attn_v': 'transformer.h.{}.attn.c_attn', 'attn_o': 'transformer.h.{}.attn.c_proj', 'ln2': 'transformer.h.{}.ln_2', 'mlp_ff1': 'transformer.h.{}.mlp.c_fc', 'mlp_ff2': 'transformer.h.{}.mlp.c_proj', 'mlp_act': 'transformer.h.{}.mlp.act', 'include_mlp_bias': True, 'include_attn_bias': True}\n",
      "filter_name_or_code: attn_only --> filtering: going to change only attn layers\n",
      "n_shots_icl: 3\n"
     ]
    }
   ],
   "source": [
    "model_extra_args = {}\n",
    "for arg in args.model_args.split(','):\n",
    "    if arg == '':\n",
    "        continue\n",
    "    k, v = arg.split('=')\n",
    "    model_extra_args[k] = v\n",
    "print(f'model_extra_args: {model_extra_args}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_extra_args).eval().requires_grad_(False).to(device)\n",
    "model_aux = llm_utils.model_extra(model=model, device=device)\n",
    "model_config = copy.deepcopy(model.config)\n",
    "config = model_aux.config\n",
    "# del model\n",
    "\n",
    "n_embd = model_aux.n_embd\n",
    "n_head = model_aux.n_head\n",
    "head_size = model_aux.head_size\n",
    "n_layer = model_aux.n_layer\n",
    "\n",
    "pad_k = model_aux.pad_k\n",
    "pad_v = model_aux.pad_v\n",
    "\n",
    "# params_names_filter = opt_utils.get_filter_by_name(args.filter_layers)\n",
    "params_names_filter = opt_utils.get_filter_by_name('attn_only')\n",
    "# params_names_filter = lambda x: ('attn' in x or 'attention' in x) and 'weight' in x\n",
    "\n",
    "# n_shots_icl = [int(x) for x in args.n_shots_icl.split(',')]\n",
    "n_shots_icl = args.n_shots_icl\n",
    "print(f'n_shots_icl: {n_shots_icl}')\n",
    "metric_to_use = args.metric_to_use\n",
    "n_samples = args.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prefix is a code for which version of the code we use in this notebook\n",
    "# previously: f'av7.0_{args.model_name.replace(\"/\", \"-\")}_{args.dataset_name}_[{args.n_shots_icl}]_{n_samples}_{metric_to_use}_{args.postfix_name}_'\n",
    "output_prefix = f'cm8_{args.model_name.replace(\"/\", \"-\")}_{args.dataset_name}_[{args.n_shots_icl}]_{n_samples}_{metric_to_use}_{args.postfix_name}_'\n",
    "\n",
    "plot_wrapper = plot_aux_wrapper(output_folder=args.out_folder, \n",
    "                            output_prefix=output_prefix,\n",
    "                            local_show=True)\n",
    "\n",
    "show_every_n_layer = 1 if n_layer <= 12 else 2 if n_layer <= 24 else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_llama = 'llama' in args.model_name or 'facebook/opt' in args.model_name\n",
    "prepend_bos = not is_llama\n",
    "\n",
    "prefixes=args.prefixes\n",
    "separators=args.separators\n",
    "\n",
    "compute_ppl=False\n",
    "shuffle_labels=False\n",
    "generate_str=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset: prev_item with 157 train, 21 valid and 47 test samples\n",
      "Example: 1) {'input': 'OO', 'output': 'NN'}\n",
      "Example: 2) {'input': 'oo', 'output': 'nn'}\n"
     ]
    }
   ],
   "source": [
    "dataset = exp_ra_utils.data_loading_wrapper(args.dataset_name, \n",
    "                                            root_data_dir=args.root_data_dir,\n",
    "                                            seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: i\\nA: h\\n\\nQ: OO\\nA: NN\\n\\nQ: oo\\nA: nn\\n\\nQ: mm\\nA: ll\\n\\nQ: r\\nA: q\\n\\nQ: XVIII\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: XVIII\\nA:'\n",
      "Input Sentence: '<|endoftext|>Q: i\\nA: h\\n\\nQ: OO\\nA: NN\\n\\nQ: oo\\nA: nn\\n\\nQ: mm\\nA: ll\\n\\nQ: r\\nA: q\\n\\nQ: XVIII\\nA:' \n",
      "\n",
      "Input Query: 'XVIII', Target: 'XVII'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' o', 0.05167), (' O', 0.04458), (' n', 0.02428), (' Q', 0.02194), (' X', 0.02174)] \n",
      "\n",
      "Input Sentence: '<|endoftext|>Q: XVIII\\nA:' \n",
      "\n",
      "Input Query: 'XVIII', Target: 'XVII'\n",
      "\n",
      "Zero-Shot Prompt Top K Vocab Probs:\n",
      " [('\\n', 0.06804), (' The', 0.06326), (' I', 0.05632), (' XV', 0.0369), (' What', 0.03228)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_example = exp_ra_utils.data_print_and_test_example(model, tokenizer=tokenizer,\n",
    "                                                            dataset=dataset,\n",
    "                                                            prepend_bos=prepend_bos,\n",
    "                                                            prefixes=prefixes,\n",
    "                                                            separators=separators,\n",
    "                                                            shuffle_labels=shuffle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_out_indirect_effect = f'{args.out_folder}/{output_prefix}_simple_casual_mediation_v2.pt'\n",
    "\n",
    "meta_data = {\n",
    "             'path_out_indirect_effect': path_out_indirect_effect,\n",
    "             'example_sentence': sentence_example,  # to verify we ran what we wanted\n",
    "             'ds_train_len': len(dataset['train']),\n",
    "             'ds_valid_len': len(dataset['valid']),\n",
    "             'ds_test_len': len(dataset['test']),\n",
    "             'run_args': str(args.__dict__),\n",
    "             'start_time': START_TIME,\n",
    "             'debug_flag_on': DEBUG_FLAG}\n",
    "meta_data_out = os.path.join(args.out_folder, f'{output_prefix}_meta_data_simple_indirect_effect_v2.json')\n",
    "if args.check_if_result_already_exists and os.path.exists(meta_data_out):\n",
    "    print(f'File already exists: {meta_data_out}. Terminating this run. If you wish to re-run, please remove the file or disable the flag --check_if_result_already_exists')\n",
    "    print(f'Run args: {args}')\n",
    "    print('Exit...')\n",
    "    exit(0)\n",
    "\n",
    "with open(meta_data_out, 'w') as f:\n",
    "    json.dump(meta_data, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_indirect_effect(example_index, split='train', annotate=False, dataset=dataset, curr_n_shots=n_shots_icl):\n",
    "    res_effect = torch.zeros(n_layer, n_head).to(torch.float64)\n",
    "\n",
    "    j = example_index\n",
    "    if curr_n_shots == 0:\n",
    "        word_pairs = {'input':[], 'output':[]}\n",
    "    else:\n",
    "        random_samples_without_current_j = np.random.choice(len(dataset[split]), curr_n_shots, replace=False)\n",
    "        while j in random_samples_without_current_j:\n",
    "            random_samples_without_current_j = np.random.choice(len(dataset[split]), curr_n_shots, replace=False)\n",
    "\n",
    "        word_pairs = dataset[split][random_samples_without_current_j]\n",
    "    word_pairs_test = dataset[split][j]\n",
    "    if prefixes is not None and separators is not None:\n",
    "        prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair = word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                shuffle_labels=shuffle_labels, prefixes=prefixes, separators=separators)\n",
    "    else:\n",
    "        prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair = word_pairs_test, prepend_bos_token=prepend_bos, shuffle_labels=shuffle_labels)\n",
    "        \n",
    "    # Get relevant parts of the Prompt\n",
    "    query, target = prompt_data['query_target']['input'], prompt_data['query_target']['output']\n",
    "    query = query[0] if isinstance(query, list) else query\n",
    "    if generate_str:\n",
    "        target = [target] if not isinstance(target, list) else target\n",
    "    else:\n",
    "        target = target[0] if isinstance(target, list) else target\n",
    "\n",
    "    sentence = [create_prompt(prompt_data)]\n",
    "\n",
    "    # Figure out tokens of interest\n",
    "    target_token_id = get_answer_id(sentence[0], target, tokenizer)[0]\n",
    "\n",
    "    device = model.device\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    original_pred_idx = len(inputs.input_ids.squeeze()) - 1\n",
    "\n",
    "    # # if compute_nll:\n",
    "    # target_completion = \"\".join(sentence + [target])\n",
    "    # nll_inputs = tokenizer(target_completion, return_tensors='pt').to(device)\n",
    "    # nll_targets = nll_inputs.input_ids.clone()\n",
    "    # target_len = len(nll_targets.squeeze()) - len(inputs.input_ids.squeeze()) \n",
    "    # nll_targets[:,:-target_len] = -100  # This is the accepted value to skip indices when computing loss in nn.CrossEntropyLoss\n",
    "\n",
    "    if annotate:\n",
    "        tmp_print = sentence[0].replace(\"\\n\", \"\\\\n\")\n",
    "        print(f'[{curr_n_shots}-shots] len={inputs.input_ids.shape[1]}, sentence_with_target={tmp_print}')\n",
    "        print(f'target_token_id: {target_token_id}')\n",
    "\n",
    "    clean_output = model(**inputs)\n",
    "    clean_probs = torch.softmax(clean_output.logits[:,-1], dim=-1).squeeze(0).to(torch.float64)  # clean prob of the last position -> the answer we expect to check\n",
    "\n",
    "    for layer_index in range(n_layer):\n",
    "        Wo = llm_utils.rgetattr(model, config.attn_o.format(layer_index))\n",
    "        original_Wo_weights = Wo.weight.clone()\n",
    "        for head_index in range(n_head):\n",
    "            tmp_Wo = original_Wo_weights.clone()\n",
    "            if type(Wo) == Conv1D:\n",
    "                tmp_Wo[head_size*head_index:head_size*(head_index+1)] = 0\n",
    "            elif type(Wo) == nn.Linear:\n",
    "                tmp_Wo[:, head_size*head_index:head_size*(head_index+1)] = 0\n",
    "            else:\n",
    "                raise ValueError(f'Unknown type of Wo: {type(tmp_Wo)} (currently only transformers.pytorch_utils.Conv1D and torch.nn.Linear are supported)')\n",
    "\n",
    "            Wo.weight[:] = tmp_Wo\n",
    "\n",
    "            indirect_output = model(**inputs)\n",
    "            indirect_probs = torch.softmax(indirect_output.logits[:,-1], dim=-1).to(torch.float64)\n",
    "\n",
    "            curr_indirect_effect = (indirect_probs - clean_probs)[0,target_token_id].squeeze().detach().cpu()\n",
    "            if res_effect[layer_index, head_index] != 0:\n",
    "                print(f'Warning: res_effect[{layer_index}, {head_index}] was not zero before the update. It was {res_effect[layer_index, head_index]}')\n",
    "            res_effect[layer_index, head_index] = curr_indirect_effect.item()\n",
    "        Wo.weight[:] = original_Wo_weights\n",
    "    return res_effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0\n",
      "[3-shots] len=36, sentence_with_target=<|endoftext|>Q: XIII\\nA: XII\\n\\nQ: iii\\nA: ii\\n\\nQ: tuesday\\nA: monday\\n\\nQ: i\\nA:\n",
      "target_token_id: 289\n",
      "j=3\n",
      "[3-shots] len=34, sentence_with_target=<|endoftext|>Q: 25\\nA: 24\\n\\nQ: CC\\nA: BB\\n\\nQ: j\\nA: i\\n\\nQ: mm\\nA:\n",
      "target_token_id: 32660\n"
     ]
    }
   ],
   "source": [
    "all_res = []\n",
    "\n",
    "for j in range(n_samples):\n",
    "    if j % 3 == 0:\n",
    "        print(f'j={j}')\n",
    "    curr_res = get_experiment_indirect_effect(example_index=j, annotate=j%3==0)\n",
    "    all_res.append(curr_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indirect_effect_res shape: torch.Size([5, 12, 12]) (n_examples, n_layer, n_head)\n",
      "Took mean result over 5 samples\n",
      "mean_indirect_effect_res shape: torch.Size([12, 12]) (n_layer, n_head)\n"
     ]
    }
   ],
   "source": [
    "indirect_effect_res = torch.stack(all_res)\n",
    "print(f'indirect_effect_res shape: {indirect_effect_res.shape} (n_examples, n_layer, n_head)')\n",
    "print(f'Took mean result over {n_samples} samples')\n",
    "mean_indirect_effect_res = indirect_effect_res.mean(dim=0)\n",
    "print(f'mean_indirect_effect_res shape: {mean_indirect_effect_res.shape} (n_layer, n_head)')\n",
    "torch.save(mean_indirect_effect_res, path_out_indirect_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(f'DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
