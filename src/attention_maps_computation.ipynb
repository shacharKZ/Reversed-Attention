{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create forward and reversed attention maps\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# another code we wrote\n",
    "sys.path.append('../')\n",
    "import llm_utils\n",
    "import exp_ra_utils\n",
    "import opt_utils\n",
    "from plot_utils import plot_aux_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from function_vectors.src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "    from function_vectors.src.utils.eval_utils import decode_to_vocab, sentence_eval, n_shot_eval_no_intervention, get_answer_id\n",
    "except Exception as error:\n",
    "    print('could not import from function_vectors package with the following error:')\n",
    "    print(error)\n",
    "    print('Make sure you first pull relevant submodules. See README.md for more info.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "# DEBUG_FLAG = True  # for local testing\n",
    "DEBUG_FLAG = False  # automatically set below\n",
    "\n",
    "try:\n",
    "    DEBUG_FLAG = torch.backends.mps.is_available()  # since only Apple M1-3 supports this, we can use this as a flag for local testing\n",
    "except:\n",
    "    pass\n",
    "if DEBUG_FLAG:\n",
    "    print('*'*40)\n",
    "    print(f'    DEBUG MODE [{START_TIME}]')\n",
    "    print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--check_if_result_already_exists'], dest='check_if_result_already_exists', nargs=0, const=True, default=False, type=None, choices=None, required=False, help=None, metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='gpt2' if DEBUG_FLAG else 'gpt2-xl')\n",
    "parser.add_argument('--model_args', type=str, default='')\n",
    "parser.add_argument('--out_folder', type=str, default='tmp_plots1')\n",
    "parser.add_argument('--postfix_name', type=str, default='')\n",
    "parser.add_argument('--disable_pad_token', action='store_true')\n",
    "parser.add_argument('--root_data_dir', type=str, default='../function_vectors/dataset_files')\n",
    "parser.add_argument('--dataset_name', type=str, default='antonym')\n",
    "parser.add_argument('--n_shots_icl', type=int, default=0)\n",
    "parser.add_argument('--n_samples', type=int, default=5 if DEBUG_FLAG else 25)\n",
    "parser.add_argument('--metric_to_use', type=str, default='f1_score')  # f1_score, exact_match_score, first_word_score\n",
    "parser.add_argument('--prefixes', help='Prompt template prefixes to be used', type=json.loads, required=False, default={\"input\":\"Q:\", \"output\":\"A:\", \"instructions\":\"\"})\n",
    "parser.add_argument('--separators', help='Prompt template separators to be used', type=json.loads, required=False, default={\"input\":\"\\n\", \"output\":\"\\n\\n\", \"instructions\":\"\"})    \n",
    "parser.add_argument('--device', type=str, default=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "parser.add_argument('--check_if_result_already_exists', action='store_true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown args: ['--f=/Users/ks/Library/Jupyter/runtime/kernel-v2-37936PB31iTO0ySOS.json']\n",
      "args: Namespace(model_name='gpt2', model_args='', out_folder='tmp_plots1', postfix_name='', disable_pad_token=False, root_data_dir='../function_vectors/dataset_files', dataset_name='antonym', n_shots_icl=0, n_samples=5, metric_to_use='f1_score', prefixes={'input': 'Q:', 'output': 'A:', 'instructions': ''}, separators={'input': '\\n', 'output': '\\n\\n', 'instructions': ''}, device=device(type='cpu'), seed=42, check_if_result_already_exists=False)\n"
     ]
    }
   ],
   "source": [
    "args, unknown = parser.parse_known_args()\n",
    "print('unknown args:', unknown)\n",
    "print('args:', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding pad token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "if not args.disable_pad_token:\n",
    "    print(f'adding pad token: {tokenizer.eos_token}')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "try:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # not blocking, just to prevent warnings and faster tokenization\n",
    "except:\n",
    "    pass\n",
    "padding_flag = tokenizer.pad_token_id is not None  # this is up to if the tokenizer was configured with padding or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu [cuda available? => False, cuda version: None, args.device = \"cpu\"]\n"
     ]
    }
   ],
   "source": [
    "if args.device == 'AUTO' or args.device == '':\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "else:\n",
    "    device = torch.device(args.device)\n",
    "print(f'Using device: {device} [cuda available? => {torch.cuda.is_available()}, cuda version: {torch.version.cuda}, args.device = \"{args.device}\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_extra_args = {}\n",
    "for arg in args.model_args.split(','):\n",
    "    if arg == '':\n",
    "        continue\n",
    "    k, v = arg.split('=')\n",
    "    model_extra_args[k] = v\n",
    "print(f'model_extra_args: {model_extra_args}')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(args.model_name, **model_extra_args).eval().requires_grad_(False).to(device)\n",
    "model_aux = llm_utils.model_extra(model=model, device=device)\n",
    "model_config = copy.deepcopy(model.config)\n",
    "config = model_aux.config\n",
    "# del model\n",
    "\n",
    "n_embd = model_aux.n_embd\n",
    "n_head = model_aux.n_head\n",
    "head_size = model_aux.head_size\n",
    "n_layer = model_aux.n_layer\n",
    "\n",
    "pad_k = model_aux.pad_k\n",
    "pad_v = model_aux.pad_v\n",
    "\n",
    "# params_names_filter = opt_utils.get_filter_by_name(args.filter_layers)\n",
    "params_names_filter = opt_utils.get_filter_by_name('attn_only')\n",
    "# params_names_filter = lambda x: ('attn' in x or 'attention' in x) and 'weight' in x\n",
    "\n",
    "n_shots_icl = args.n_shots_icl\n",
    "print(f'n_shots_icl: {n_shots_icl}')\n",
    "metric_to_use = args.metric_to_use\n",
    "n_samples = args.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the prefix is a code for which version of the code we use in this notebook\n",
    "output_prefix = f'av1_{args.model_name.replace(\"/\", \"-\")}_{args.dataset_name}_[{args.n_shots_icl}]_{n_samples}_{metric_to_use}_{args.postfix_name}_'\n",
    "\n",
    "plot_wrapper = plot_aux_wrapper(output_folder=args.out_folder, \n",
    "                            output_prefix=output_prefix,\n",
    "                            local_show=True)\n",
    "\n",
    "show_every_n_layer = 1 if n_layer <= 12 else 2 if n_layer <= 24 else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_llama = 'llama' in args.model_name or 'facebook/opt' in args.model_name\n",
    "prepend_bos = not is_llama\n",
    "\n",
    "prefixes=args.prefixes\n",
    "separators=args.separators\n",
    "\n",
    "compute_ppl=False\n",
    "shuffle_labels=False\n",
    "generate_str=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load dataset: antonym with 1678 train, 216 valid and 504 test samples\n",
      "Example: 1) {'input': 'lesbian', 'output': 'straight'}\n",
      "Example: 2) {'input': 'homegrown', 'output': 'imported'}\n"
     ]
    }
   ],
   "source": [
    "dataset = exp_ra_utils.data_loading_wrapper(args.dataset_name, \n",
    "                                            root_data_dir=args.root_data_dir,\n",
    "                                            seed=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: noise\\nA: silence\\n\\nQ: lesbian\\nA: straight\\n\\nQ: homegrown\\nA: imported\\n\\nQ: default\\nA: customized\\n\\nQ: disrespect\\nA: respect\\n\\nQ: damn\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: damn\\nA:'\n",
      "Input Sentence: '<|endoftext|>Q: noise\\nA: silence\\n\\nQ: lesbian\\nA: straight\\n\\nQ: homegrown\\nA: imported\\n\\nQ: default\\nA: customized\\n\\nQ: disrespect\\nA: respect\\n\\nQ: damn\\nA:' \n",
      "\n",
      "Input Query: 'damn', Target: 'bless'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' damn', 0.06769), (' disrespect', 0.02297), (' I', 0.01337), (' stupid', 0.01174), (' insult', 0.01142)] \n",
      "\n",
      "Input Sentence: '<|endoftext|>Q: damn\\nA:' \n",
      "\n",
      "Input Query: 'damn', Target: 'bless'\n",
      "\n",
      "Zero-Shot Prompt Top K Vocab Probs:\n",
      " [(' I', 0.08618), ('\\n', 0.02441), (' The', 0.01667), (' It', 0.01592), (' Yeah', 0.01445)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_example = exp_ra_utils.data_print_and_test_example(model, tokenizer=tokenizer,\n",
    "                                                            dataset=dataset,\n",
    "                                                            prepend_bos=prepend_bos,\n",
    "                                                            prefixes=prefixes,\n",
    "                                                            separators=separators,\n",
    "                                                            shuffle_labels=shuffle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forward_path = f'{args.out_folder}/{output_prefix}_forward_attn_maps_norms'\n",
    "df_backward_path = f'{args.out_folder}/{output_prefix}_backward_attn_maps_norms'\n",
    "\n",
    "meta_data = {\n",
    "             'df_forward_path': df_forward_path,\n",
    "             'df_backward_path': df_backward_path,\n",
    "             'example_sentence': sentence_example,  # to verify we ran what we wanted\n",
    "             'ds_train_len': len(dataset['train']),\n",
    "             'ds_valid_len': len(dataset['valid']),\n",
    "             'ds_test_len': len(dataset['test']),\n",
    "             'start_time': START_TIME,\n",
    "             'run_args': str(args),\n",
    "             'debug_flag_on': DEBUG_FLAG}\n",
    "meta_data_out = os.path.join(args.out_folder, f'{output_prefix}_get_mean_attention_tables.json')\n",
    "if args.check_if_result_already_exists and os.path.exists(meta_data_out):\n",
    "    print(f'File already exists: {meta_data_out}. Terminating this run. If you wish to re-run, please remove the file or disable the flag --check_if_result_already_exists')\n",
    "    print(f'Run args: {args}')\n",
    "    print('Exit...')\n",
    "    exit(0)\n",
    "\n",
    "with open(meta_data_out, 'w') as f:\n",
    "    json.dump(meta_data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params to optimize: 24\n",
      "Going to edit only the following layers (showing only the first 10): ['transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.weight']\n",
      "Going to edit only the following layers (showing only the last 10): ['transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "params_names = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if params_names_filter(param_name):\n",
    "        param.requires_grad_(True)  # only relevant layer are trained (all the rest are freezed)\n",
    "        params.append(param)\n",
    "        params_names.append(param_name)\n",
    "print(f'Number of params to optimize: {len(params)}')\n",
    "print('Going to edit only the following layers (showing only the first 10):', params_names[:10])\n",
    "print('Going to edit only the following layers (showing only the last 10):', params_names[-10:])\n",
    "\n",
    "opt = torch.optim.SGD(params, lr=1e-3, weight_decay=0)  # learning rate does not really matter as we only intrest in the VJPs which we collect\n",
    "min_loss_for_update = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_lm_config = [config.attn_o]  # the only type of layers we need to calculate the reverse attn (witht he attn values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiment_collector(example_index, split='train', annotate=False, dataset=dataset, curr_n_shots=n_shots_icl):\n",
    "    collector = {}\n",
    "    j = example_index\n",
    "    # hs_collector = llm_utils.wrap_model(model, layers_to_check=args.llm_config_path, \n",
    "    #                                     return_hooks_handler=True, forward=True, max_len=1000)\n",
    "    # grad_collector = llm_utils.wrap_model(model, layers_to_check=args.llm_config_path, \n",
    "    #                                     return_hooks_handler=True, forward=False, max_len=1000)\n",
    "    hs_collector = {}\n",
    "    grad_collector = llm_utils.wrap_model(model, layers_to_check=short_lm_config, \n",
    "                                        return_hooks_handler=True, forward=False, max_len=1000)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    if curr_n_shots == 0:\n",
    "        word_pairs = {'input':[], 'output':[]}\n",
    "    else:\n",
    "        random_samples_without_current_j = np.random.choice(len(dataset[split]), curr_n_shots, replace=False)\n",
    "        while j in random_samples_without_current_j:\n",
    "            random_samples_without_current_j = np.random.choice(len(dataset[split]), curr_n_shots, replace=False)\n",
    "\n",
    "        word_pairs = dataset[split][random_samples_without_current_j]\n",
    "    word_pairs_test = dataset[split][j]\n",
    "    if prefixes is not None and separators is not None:\n",
    "        prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair = word_pairs_test, prepend_bos_token=prepend_bos, \n",
    "                                                shuffle_labels=shuffle_labels, prefixes=prefixes, separators=separators)\n",
    "    else:\n",
    "        prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair = word_pairs_test, prepend_bos_token=prepend_bos, shuffle_labels=shuffle_labels)\n",
    "        \n",
    "    # Get relevant parts of the Prompt\n",
    "    query, target = prompt_data['query_target']['input'], prompt_data['query_target']['output']\n",
    "    query = query[0] if isinstance(query, list) else query\n",
    "    if generate_str:\n",
    "        target = [target] if not isinstance(target, list) else target\n",
    "    else:\n",
    "        target = target[0] if isinstance(target, list) else target\n",
    "    \n",
    "    sentence = [create_prompt(prompt_data)]\n",
    "    \n",
    "    # Figure out tokens of interest\n",
    "    target_token_id = get_answer_id(sentence[0], target, tokenizer)\n",
    "\n",
    "    device = model.device\n",
    "    inputs = tokenizer(sentence, return_tensors='pt').to(device)\n",
    "    original_pred_idx = len(inputs.input_ids.squeeze()) - 1\n",
    "     \n",
    "    # REF1: https://github.com/ericwtodd/function_vectors/blob/1e8a9a0f3583c547efcee2b4add4e880c25a96d3/src/utils/intervention_utils.py#L151\n",
    "    # REF2: https://huggingface.co/docs/transformers/en/perplexity\n",
    "    # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "    # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "    # to the left by 1.\n",
    "    target_completion = \"\".join(sentence + [target])\n",
    "    nll_inputs = tokenizer(target_completion, return_tensors='pt').to(device)\n",
    "    nll_targets = nll_inputs.input_ids.clone()\n",
    "    target_len = len(nll_targets.squeeze()) - len(inputs.input_ids.squeeze()) \n",
    "    nll_targets[:,:-target_len] = -100  # This is the accepted value to skip indices when computing loss in nn.CrossEntropyLoss\n",
    "\n",
    "    if annotate:\n",
    "        tmp_print = target_completion.replace(\"\\n\", \"\\\\n\")\n",
    "        print(f'[{curr_n_shots}-shots] len={nll_inputs.input_ids.shape[1]}, sentence_with_target={tmp_print}')\n",
    "\n",
    "    output = model(**nll_inputs, labels=nll_targets, output_attentions=True, output_hidden_states=True, use_cache=True)\n",
    "    \n",
    "    # compute gradients but do not apply the step of the optimizer\n",
    "    # if clean_nll >= min_loss_for_update:\n",
    "    # clean_nll = output.loss.item()\n",
    "    # clean_output = output.logits[:,original_pred_idx,:]\n",
    "    output.loss.backward()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    collector[curr_n_shots] = {\n",
    "        # 'hs': copy.deepcopy(hs_collector),\n",
    "        'grad': copy.deepcopy(grad_collector),\n",
    "        'kv_cache': output.past_key_values,\n",
    "        'attentions': output.attentions,\n",
    "        # 'inputs': copy.deepcopy(nll_inputs)\n",
    "    }\n",
    "\n",
    "    llm_utils.remove_collector_hooks(hs_collector)\n",
    "    llm_utils.remove_collector_hooks(grad_collector)\n",
    "    return collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j=0\n",
      "[0-shots] len=8, sentence_with_target=<|endoftext|>Q: noise\\nA: silence\n"
     ]
    }
   ],
   "source": [
    "# also could cahced all collectors but it is not feasible with big models and many examples (one collector of gpt2-XL can get up to 500MB)\n",
    "\n",
    "all_forward_attns = {}\n",
    "all_rev_attns = {}\n",
    "\n",
    "for layer_index in range(n_layer):\n",
    "    all_rev_attns[layer_index] = {}\n",
    "    all_forward_attns[layer_index] = {}\n",
    "    for head_index in range(n_head):\n",
    "        all_rev_attns[layer_index][head_index] = []\n",
    "        all_forward_attns[layer_index][head_index] = []\n",
    "\n",
    "for j in range(n_samples):\n",
    "    if j % 10 == 0:\n",
    "        print(f'j={j}')\n",
    "    collector = get_experiment_collector(example_index=j, annotate=j%10==0)\n",
    "\n",
    "    for layer_index in range(n_layer):\n",
    "        for head_index in range(n_head):\n",
    "            forward_attn_map, rev_attn_map = exp_ra_utils.get_forward_and_reversed_attn(collector[n_shots_icl], layer_index, head_index, config, head_size)\n",
    "            all_rev_attns[layer_index][head_index].append(rev_attn_map)\n",
    "            all_forward_attns[layer_index][head_index].append(forward_attn_map)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all tables as csv in tmp_plots1/av1_gpt2_antonym_[0]_5_f1_score___<table-name>.csv\n"
     ]
    }
   ],
   "source": [
    "forward_attn_maps_norms_mean = []\n",
    "backward_attn_maps_norms_mean = []\n",
    "forward_attn_maps_norms_std = []\n",
    "backward_attn_maps_norms_std = []\n",
    "for layer_index in range(n_layer):\n",
    "    forward_attn_maps_norms_mean.append([])\n",
    "    backward_attn_maps_norms_mean.append([])\n",
    "    forward_attn_maps_norms_std.append([])\n",
    "    backward_attn_maps_norms_std.append([])\n",
    "    forward_attn_maps_norms_mean[layer_index] = []\n",
    "    backward_attn_maps_norms_mean[layer_index] = []\n",
    "    forward_attn_maps_norms_std[layer_index] = []\n",
    "    backward_attn_maps_norms_std[layer_index] = []\n",
    "    for head_index in range(n_head):\n",
    "        # mean_rev_attn = sum([x.norm() for x in all_rev_attns[layer_index][head_index]]) / len(all_rev_attns[layer_index][head_index])\n",
    "        # mean_forward_attn = sum([x.norm() for x in all_forward_attns[layer_index][head_index]]) / len(all_forward_attns[layer_index][head_index])\n",
    "        mean_rev_attn = np.mean([x.norm().item() for x in all_rev_attns[layer_index][head_index]], axis=-1)\n",
    "        mean_forward_attn = np.mean([x.norm().item() for x in all_forward_attns[layer_index][head_index]], axis=-1)\n",
    "        forward_attn_maps_norms_mean[layer_index].append(mean_forward_attn)\n",
    "        backward_attn_maps_norms_mean[layer_index].append(mean_rev_attn)\n",
    "        std_rev_attn = np.std([x.norm().item() for x in all_rev_attns[layer_index][head_index]], axis=-1)\n",
    "        std_forward_attn = np.std([x.norm().item() for x in all_forward_attns[layer_index][head_index]], axis=-1)\n",
    "        forward_attn_maps_norms_std[layer_index].append(std_forward_attn)\n",
    "        backward_attn_maps_norms_std[layer_index].append(std_rev_attn)\n",
    "        \n",
    "\n",
    "forward_attn_maps_norms_mean = np.array(forward_attn_maps_norms_mean).T\n",
    "backward_attn_maps_norms_mean = np.array(backward_attn_maps_norms_mean).T\n",
    "forward_attn_maps_norms_std = np.array(forward_attn_maps_norms_std).T\n",
    "backward_attn_maps_norms_std = np.array(backward_attn_maps_norms_std).T\n",
    "\n",
    "# save all tables as df\n",
    "df_forward_attn_maps_norms_mean = pd.DataFrame(forward_attn_maps_norms_mean, columns=[f'layer_{i}' for i in range(n_layer)], index=[f'head_{i}' for i in range(n_head)])\n",
    "df_backward_attn_maps_norms_mean = pd.DataFrame(backward_attn_maps_norms_mean, columns=[f'layer_{i}' for i in range(n_layer)], index=[f'head_{i}' for i in range(n_head)])\n",
    "df_forward_attn_maps_norms_std = pd.DataFrame(forward_attn_maps_norms_std, columns=[f'layer_{i}' for i in range(n_layer)], index=[f'head_{i}' for i in range(n_head)])\n",
    "df_backward_attn_maps_norms_std = pd.DataFrame(backward_attn_maps_norms_std, columns=[f'layer_{i}' for i in range(n_layer)], index=[f'head_{i}' for i in range(n_head)])\n",
    "print(f'Saving all tables as csv in {args.out_folder}/{output_prefix}_<table-name>.csv')\n",
    "df_forward_attn_maps_norms_mean.to_csv(f'{df_forward_path}_mean.csv')\n",
    "df_backward_attn_maps_norms_mean.to_csv(f'{df_backward_path}_mean.csv')\n",
    "df_forward_attn_maps_norms_std.to_csv(f'{df_forward_path}_std.csv')\n",
    "df_backward_attn_maps_norms_std.to_csv(f'{df_backward_path}_std.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')\n",
    "\n",
    "raise ValueError('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for norm_table, table_name in [(forward_attn_maps_norms_mean, 'forward mean'), \n",
    "                                (forward_attn_maps_norms_std, 'forward std'),\n",
    "                                (backward_attn_maps_norms_mean, 'reversed mean'),\n",
    "                                (backward_attn_maps_norms_std, 'reversed std')]:\n",
    "    \n",
    "    if n_layer > n_head:\n",
    "        ratio = 1.5 * n_layer / n_head\n",
    "    else:\n",
    "        ratio = 2.5\n",
    "    fig, ax = plt.subplots(figsize=(ratio * 3, 3))\n",
    "    cmap = 'Purples' if 'mean' in table_name else 'Greys'\n",
    "    sns.heatmap(norm_table, ax=ax, annot=False, fmt=\".2f\", cmap=cmap, vmin=0, cbar_kws={'label': 'norm'})\n",
    "    ax.set_xticks(np.arange(0, n_layer, show_every_n_layer) + 0.5)\n",
    "    ax.set_yticks(np.arange(0, n_head, max(1, show_every_n_layer//2)) + 0.5)\n",
    "    ax.set_xticklabels(np.arange(0, n_layer, show_every_n_layer), rotation=0)\n",
    "    ax.set_yticklabels(np.arange(0, n_head, max(1, show_every_n_layer//2)), rotation=0)\n",
    "    ax.set_xlabel('layer index')\n",
    "    ax.set_ylabel('head index')\n",
    "    for _, spine in ax.spines.items(): \n",
    "        spine.set_visible(True) \n",
    "        spine.set_linewidth(1)\n",
    "    plt.tight_layout()\n",
    "    plot_wrapper(plt, title=f'{table_name} attention map norms', save_also_without_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
