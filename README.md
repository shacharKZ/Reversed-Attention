# Reversed Attention

## Table of Contents
1. [Description](#description)
2. [Installation](#installation)
3. [Usage](#usage)

## Description
This repo will contain the official code for the paper: Reversed Attention: On The Gradient Descent Of Attention Layers in GPT

Meanwhile, we make the following demo available: [![Colab Reversed Attention](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/13HDQ6o-TN7PcKCk4DlKgQ9O6jEeHbUW0?usp=sharing)


## Installation

### Prerequisites
Make sure you have Python 3.10 installed on your machine.

All files must be in the same folder

### Installing Dependencies
In your Python environment, install the required dependencies using `pip` and the `requirements.txt` file:

```sh
pip install -r requirements.txt
```

## Usage

Make sure all files are in the same folder and run `main_demo.ipynb`.

This script demonstrates how to obtain the backward pass attention, which we named Reversed Attention.

This demo can be run in Colab and does not require a GPU (but it can make the run faster)


## TODO

[] Release code.


